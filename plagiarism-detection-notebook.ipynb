{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import spacy\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import KMeans\n",
    "lemma = nltk.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "n_most_common = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "papers = pd.read_csv('papers.csv').apply(lambda x: x.astype(str).str.lower())\n",
    "#make data frame which has only title and text columns\n",
    "papers_reduced = papers.filter(['title','paper_text'], axis=1)[0:50]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide papers into sentences\n",
    "papers_reduced['paper_text_sentences'] = papers_reduced['paper_text'].apply(lambda x: sent_tokenize(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function which operates on a single sentences, cleans its stopwords, \n",
    "#lemmatizes it, removes numbers and finally vectorizes it\n",
    "def process_sent(sent):\n",
    "    tokenized = nltk.word_tokenize(sent)\n",
    "    cleaned = [word for word in tokenized if word not in stopwords.words('english') and word.isalpha() and len(word)>5]\n",
    "    lemmatized = list(map(lambda x: lemma.lemmatize(x), cleaned))\n",
    "    joined_back = ' '.join(lemmatized)\n",
    "    nlp_joined_back = nlp(joined_back)\n",
    "    return nlp(joined_back)\n",
    "#create empty dataframe to hold all sentences (string and vector format) \n",
    "#along with names of papers they are coming from\n",
    "papers_dataframe = pd.DataFrame(columns=['paper_name', 'sentence_vectors', 'sentences'])\n",
    "\n",
    "#iterate through all the papers to fill papers_dataframe\n",
    "for row in papers_reduced.itertuples():\n",
    "    for sent in row.paper_text_sentences:\n",
    "        vect = process_sent(sent)\n",
    "        new_df = pd.DataFrame([[row.title, vect.vector, vect]], columns=['paper_name', 'sentence_vectors', 'sentences'])\n",
    "        papers_dataframe = papers_dataframe.append(new_df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for initial centroids we generate sentences from n_most_common most common words in each paper\n",
    "all_papers = papers_reduced[\"paper_text\"]\n",
    "initial_centroids_sent = []\n",
    "#tokenize each word of each paper, find n_most_common ones and join them to create sentences\n",
    "for p in all_papers:\n",
    "    tokenized = nltk.word_tokenize(p)\n",
    "    cleaned = [word for word in tokenized if word not in stopwords.words('english') and word.isalpha() and len(word)>7]\n",
    "    lemmatized = list(map(lambda x: lemma.lemmatize(x), cleaned))\n",
    "    fd = nltk.FreqDist(lemmatized)\n",
    "    most_common_tuples = fd.most_common(n_most_common)\n",
    "    most_common_list = list(map(lambda x: x[0], most_common_tuples))\n",
    "    initial_centroids_sent.append(' '.join(most_common_list))\n",
    "#vectorize the sentences which will be initial centroids\n",
    "vectorized_centroids = np.asarray(list(map(lambda x: nlp(x).vector, initial_centroids_sent)))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=50,max_iter=300, init=vectorized_centroids, n_init=1)\n",
    "km = kmeans.fit(papers_dataframe['sentence_vectors'].tolist())\n",
    "x = km.fit_predict(papers_dataframe['sentence_vectors'].tolist())\n",
    "papers_dataframe[\"cluster\"]= x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_names = list(set(papers_dataframe['paper_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 31.2\n",
      "mode: 33\n"
     ]
    }
   ],
   "source": [
    "#check in how many different clusters one paper appears in\n",
    "names_number_of_clusters = []\n",
    "for n in paper_names:\n",
    "    names_clusters = papers_dataframe.loc[papers_dataframe['paper_name'] == n]['cluster']\n",
    "    names_number_of_clusters.append(len(collections.Counter(names_clusters).keys()))\n",
    "\n",
    "    \n",
    "#number of clusters for each paper (mean, median mode)\n",
    "import statistics\n",
    "mean_names_number_of_clusters = statistics.mean(names_number_of_clusters)\n",
    "print(\"mean: \"+str(mean_names_number_of_clusters))\n",
    "mode_names_number_of_clusters = statistics.mode(names_number_of_clusters)\n",
    "print(\"mode: \"+str(mode_names_number_of_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the mean average of different clusters which sentences of a paper appear in is 31.2. Ideally, we would like for the sentences to appear in 1-2 papers, which leads to conclusion that this method isn't viable for plagiarism detection in a set of papers. Ideas for improvement can be found in the README file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
