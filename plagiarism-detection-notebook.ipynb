{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import spacy\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "papers = pd.read_csv('papers.csv').apply(lambda x: x.astype(str).str.lower())\n",
    "#make data frame which has only title and text columns\n",
    "papers_reduced = papers.filter(['title','paper_text'], axis=1)[0:50]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide papers into sentences\n",
    "papers_reduced['paper_text_sentences'] = papers_reduced['paper_text'].apply(lambda x: sent_tokenize(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = nltk.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "#function which operates on a single sentences, cleans its stopwords, \n",
    "#lemmatizes it, removes number and finally vectorizes it\n",
    "def process_sent(sent):\n",
    "    tokenized = nltk.word_tokenize(sent)\n",
    "    cleaned = [word for word in tokenized if word not in stopwords.words('english') and word.isalpha() and len(word)>5]\n",
    "    lemmatized = list(map(lambda x: lemma.lemmatize(x), cleaned))\n",
    "    \n",
    "    joined_back = ' '.join(lemmatized)\n",
    "    nlp_joined_back = nlp(joined_back)\n",
    "    return nlp(joined_back)\n",
    "#create empty dataframe\n",
    "papers_dataframe = pd.DataFrame(columns=['paper_name', 'sentence_vectors', 'sentences'])\n",
    "\n",
    "#iterate through all the papers\n",
    "for row in papers_reduced.itertuples():\n",
    "    for sent in row.paper_text_sentences:\n",
    "        vect = process_sent(sent)\n",
    "        new_df = pd.DataFrame([[row.title, vect.vector, vect]], columns=['paper_name', 'sentence_vectors', 'sentences'])\n",
    "        papers_dataframe = papers_dataframe.append(new_df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for initial centroids we generate sentences from 10 most common words in each paper\n",
    "all_papers = papers_reduced[\"paper_text\"]\n",
    "initial_centroids_sent = []\n",
    "for p in all_papers:\n",
    "    tokenized = nltk.word_tokenize(p)\n",
    "    cleaned = [word for word in tokenized if word not in stopwords.words('english') and word.isalpha() and len(word)>7]\n",
    "    lemmatized = list(map(lambda x: lemma.lemmatize(x), cleaned))\n",
    "    fd = nltk.FreqDist(lemmatized)\n",
    "    most_common_tuples = fd.most_common(50)\n",
    "    most_common_list = list(map(lambda x: x[0], most_common_tuples))\n",
    "    initial_centroids_sent.append(' '.join(most_common_list))\n",
    "\n",
    "vectorized_centroids = np.asarray(list(map(lambda x: nlp(x).vector, initial_centroids_sent)))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "print(type(initial_centroids[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_names = set(papers_dataframe['paper_name'].tolist())\n",
    "initial_centroids = []\n",
    "\n",
    "for name in paper_names:\n",
    "    sentences = papers_dataframe.loc[papers_dataframe['paper_name'] == name]['sentence_vectors'].tolist()\n",
    "    initial_centroids.append(sentences[int(len(sentences)/2)])\n",
    "initial_centroids = np.asarray(initial_centroids)\n",
    "kmeans = KMeans(n_clusters=50,max_iter=300, init=vectorized_centroids, n_init=1)\n",
    "# papers_dataframe.reset_index( inplace=True)\n",
    "km = kmeans.fit(papers_dataframe['sentence_vectors'].tolist())\n",
    "x = km.fit_predict(papers_dataframe['sentence_vectors'].tolist())\n",
    "papers_dataframe[\"Cluster\"]= x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a model of transparent motion and non-transparent motion aftereffects', 'the gamma mlp for speech phoneme recognition', 'a dynamical model of context dependencies for the vestibulo-ocular reflex', 'a computational model of prefrontal cortex function', 'iceg morphology classification using an analogue vlsi neural network', 'ocular dominance and patterned lateral connections in a self-organizing model of the primary visual cortex', 'storing covariance by the associative long-term potentiation and depression of synaptic strengths in the hippocampus', 'associative decorrelation dynamics: a theory of self-organization and optimization in feedback networks', 'pulsestream synapses with non-volatile analogue amorphous-silicon memories', 'a mean field theory of layer iv of visual cortex and its application to artificial neural networks', 'quadratic-type lyapunov functions for competitive neural networks with different time-scales', 'exponentially many local minima for single neurons', 'correlated neuronal response: time scales and mechanisms', 'onset-based sound segmentation', 'plasticity-mediated competitive learning'}\n"
     ]
    }
   ],
   "source": [
    "print(set(papers_dataframe.loc[papers_dataframe['Cluster'] == 2]['paper_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 15, 46, 15, 46, 48, 15, 22, 1, 46, 12, 39, 39, 36, 48, 32, 22, 15, 15, 4, 23, 47, 36, 47, 11, 44, 23, 10, 22, 43, 43, 12, 44, 20, 19, 38, 20, 47, 32, 39, 22, 44, 15, 24, 4, 9, 36, 14, 8, 46, 25, 1, 39, 22, 39, 22, 39, 19, 22, 44, 33, 12, 44, 22, 46, 8, 14, 44, 8, 32, 19, 15, 24, 15, 46, 46, 34, 33, 9, 9, 44, 22, 12, 33, 44, 46, 35, 12, 41, 15, 24, 23, 44, 19, 32, 44, 46, 5, 15, 5, 22, 22, 44, 44, 46, 46, 22, 14, 47, 47, 19, 3, 12, 22, 12, 46, 12, 12, 22, 46, 17, 31, 19, 15, 15, 33, 11, 15, 36, 15, 10, 22, 15, 24, 35, 15, 19, 33, 35, 32, 22, 44, 46, 33, 12, 1, 17, 38, 12, 22, 31, 46, 46, 15, 31, 23, 3, 44, 46, 29, 26, 45, 22, 18, 1, 29, 45]\n"
     ]
    }
   ],
   "source": [
    "print(papers_dataframe.loc[papers_dataframe['paper_name'] == name]['Cluster'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = [15, 15, 46, 15, 46, 48, 15, 22, 1, 46, 12, 39, 39, 36, 48, 32, 22, 15, 15, 4, 23, 47, 36, 47, 11, 44, 23, 10, 22, 43, 43, 12, 44, 20, 19, 38, 20, 47, 32, 39, 22, 44, 15, 24, 4, 9, 36, 14, 8, 46, 25, 1, 39, 22, 39, 22, 39, 19, 22, 44, 33, 12, 44, 22, 46, 8, 14, 44, 8, 32, 19, 15, 24, 15, 46, 46, 34, 33, 9, 9, 44, 22, 12, 33, 44, 46, 35, 12, 41, 15, 24, 23, 44, 19, 32, 44, 46, 5, 15, 5, 22, 22, 44, 44, 46, 46, 22, 14, 47, 47, 19, 3, 12, 22, 12, 46, 12, 12, 22, 46, 17, 31, 19, 15, 15, 33, 11, 15, 36, 15, 10, 22, 15, 24, 35, 15, 19, 33, 35, 32, 22, 44, 46, 33, 12, 1, 17, 38, 12, 22, 31, 46, 46, 15, 31, 23, 3, 44, 46, 29, 26, 45, 22, 18, 1, 29, 45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({15: 18, 22: 18, 46: 17, 44: 14, 12: 11, 19: 7, 39: 6, 33: 6, 32: 5, 47: 5, 1: 4, 36: 4, 23: 4, 24: 4, 9: 3, 14: 3, 8: 3, 35: 3, 31: 3, 48: 2, 4: 2, 11: 2, 10: 2, 43: 2, 20: 2, 38: 2, 5: 2, 3: 2, 17: 2, 29: 2, 45: 2, 25: 1, 34: 1, 41: 1, 26: 1, 18: 1})\n"
     ]
    }
   ],
   "source": [
    "print(collections.Counter(ll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "print(len(set(papers_dataframe.loc[papers_dataframe['Cluster'] == 15]['paper_name'].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
